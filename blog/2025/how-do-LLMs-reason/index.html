<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> How do LLMs reason? | Ante Javor </title> <meta name="author" content="Ante Javor"> <meta name="description" content="Understanding what reasoning is and how LLMs can reason."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://antejavor.github.io/blog/2025/how-do-LLMs-reason/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ante</span> Javor </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">How do LLMs reason?</h1> <p class="post-meta"> Created in April 04, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/llms"> <i class="fa-solid fa-hashtag fa-sm"></i> LLMs,</a>   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   ·   <a href="/blog/category/ai"> <i class="fa-solid fa-tag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="the-mighty-llms">The mighty LLMs</h2> <p>We live in fascinating times of mighty LLMs that can help boost productivity and help with day-to-day tasks. The speed of adoption of LLMs is a story that will constantly be compared to everything that came before and after the LLMs.</p> <p>It is essential to highlight and understand the fundamentals and flaws of this technology. Let’s jump to the big question around LLMs that became quite controversial, <strong>can they reason?</strong> My short answer is <strong>no</strong>. Let’s dive into why.</p> <h2 id="reasoning">Reasoning</h2> <p>To even talk about the reasoning, we would first have to define it and agree on what reasoning is. Let’s distill this into something we refer to as a typical thinking process where you are presented with some information and can think about that information and conclude something.</p> <p>In the practical example, it would be something like this: <strong>The S&amp;P 500 sank 15% percent in the three months</strong>.</p> <p>Now, in your brain, you start to ponder that information and connect it to the events that happened in that period. Some notable things that could pop into your mind are the Trump tariffs, lower-than-expected BDP in the US economy, global conflicts, etc.</p> <p>In a pure form, this is reasoning about specific information in your brain. If I follow up on the question and ask: “What will the stock market do tomorrow?” you will probably start thinking hard about this.</p> <p>In other words, you would read projections, news, and global tax and tariff policies and invest time and effort into trying to answer this. You would construct a thesis that works under different assumptions.</p> <p>Usually, the more effort you invest, the higher the probability you will be correct.</p> <p>In a pure form, it is reasoning on the information you were given, had in your memory or acquired over the web.</p> <p>The nice thing about reasoning is that our brain is the ultimate reasoning powerhouse (most of the time :D), and it runs on beer and bananas, it does not require <a href="https://www.theverge.com/24066646/ai-electricity-energy-watts-generative-consumption" rel="external nofollow noopener" target="_blank">1 Megawatt of electricity to be trained</a></p> <h2 id="reasoning-and-llms">Reasoning and LLMs</h2> <p>LLMs <strong>cannot</strong> reason, they can <strong>mimic</strong> reasoning that sometimes works and sometimes does not. You can call it primitive, low reasoning, or use any other way to package it, but it is far away from your reasoning capabilities.</p> <p>This starts with the basis of what LLMs actually are, they are a super capable text predictor that has a lot of memory. Memory is constructed based on what they have been trained on, and small “thinking memory” is the context you are passing with your question.</p> <p>Let’s put this into practical example if your LLM was trained on this data:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>We have different colors for objects: blue, red, green, purple, yellow, etc.
The ball can be red.
The ball can be blue.
The car can be purple.
The flag can be purple.
</code></pre></div></div> <p>Now, if I ask: “What color can a ball be? Give me three answers.”</p> <p>The LLM computation will give a higher probability to the obvious answers:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. The ball can be red.
2. The ball can be blue.
3. The Ball can be ...
</code></pre></div></div> <p>The third answer would be some of the possible colors, because more objects are purple and purple has a higher frequency, it would probably give the third answer as purple. But we all know that it can be any color since it is not specified.</p> <p>If you want more details about this, one of the best explainers about why LLMs is a text predictor, I would recommend jumping on the 3Blue1Brown channel and watching a <a href="https://youtu.be/aircAruvnKk?si=6akRbRcsc2nOE_G2" rel="external nofollow noopener" target="_blank">series about neural networks</a>, specifically about the LLMs and transformers.</p> <p>By design, they are not made to think but rather to predict the next word (token, to be exact).</p> <p>Well <a href="https://platform.openai.com/docs/guides/reasoning?api-mode=chat" rel="external nofollow noopener" target="_blank">OpenAI docs seem to suggest otherwise</a>.</p> <p>This is a snippet from their docs page:</p> <blockquote> <p>Reasoning models, like OpenAI o1 and o3-mini, are new large language models trained with reinforcement learning to perform complex reasoning. Reasoning models think before they answer, producing a long internal chain of thought before responding to the user. Reasoning models excel in complex problem solving, coding, scientific reasoning, and multi-step planning for agentic workflows.</p> </blockquote> <p>The image from the same docs page illustrates this process: <img src="/assets/img/blog-how-llms-reasone/reasoning_flow.png" alt="reasoning" width="900"></p> <p>They are saying their models can perform complex reasoning.</p> <p>Well, they do not reason. They generate a thesis on how to answer questions, summarize that generation into reasoning and output, and chain this multiple times, which improves the text generation and makes it more successful. Something you will hear as <strong>Chain of Thoughts</strong>.</p> <p>So, they are <strong>mimicking</strong> reasoning where you generate text related to the text you are asking for to try to come up with a solution. It is like a person saying the facts out loud when someone asks you a question or creates a theory about something and uses theory construction to present the facts. At any step of the process, you are hoping the theory won’t go sideways.</p> <p>If we go back to the LLM that was trained on ball colors:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Question asked: What color can a ball be?

Chain of thought:
The ball can be blue
The ball can be red

Output:

Therefore, the ball can be red or blue.
</code></pre></div></div> <p>This would be a good Chain of Thoughts:</p> <p>Here is how it could fail:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Question asked: What color can a ball be? Give me three answers
Chain of thought:

The ball can be blue
The ball can be red
The car can be purple.


Therefore, the ball can be blue, red, and, I guess, purple.


</code></pre></div></div> <p>If you do not trust my examples, give a shot to your <strong>reasoning</strong> LLM, I got this on ChatGPT with reasoning:</p> <p><img src="/assets/img/blog-how-llms-reasone/chat-gpt-reasoning.png" alt="chat-gpt-reasoning" width="900"></p> <p>If you do not trust me, one of the best men to describe this is <a href="https://www.youtube.com/watch?v=N09C6oUQX5M" rel="external nofollow noopener" target="_blank">Yann LeCunn</a>, Chief AI Scientist at Meta.</p> <p>I admire the engineering behind modern LLMs and find them helpful. If you provide the proper context, they won’t hallucinate. That is why we, in <a href="https://memgraph.com/docs/ai-ecosystem" rel="external nofollow noopener" target="_blank">Memgraph</a> are focused on building cool Graph and LLM-powered AI apps.</p> <p>As they say, the harder it is to explain and argue how bad or incapable technology is, the better it is. <a href="https://en.wikipedia.org/wiki/Clarke%27s_three_laws" rel="external nofollow noopener" target="_blank">Arthur C. Clarke’s Third Law</a>:</p> <blockquote> <p>Any sufficiently advanced technology is indistinguishable from magic.</p> </blockquote> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Ante Javor. Last updated: April 04, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-BPG5KWCD0S"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-BPG5KWCD0S");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-how-do-llms-reason",title:"How do LLMs reason?",description:"Understanding what reasoning is and how LLMs can reason.",section:"Posts",handler:()=>{window.location.href="/blog/2025/how-do-LLMs-reason/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/antejavor","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/antejavor","_blank")}},{id:"socials-stackoverflow",title:"Stackoverflow",section:"Socials",handler:()=>{window.open("https://stackoverflow.com/users/9185287","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>