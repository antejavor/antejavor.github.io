<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://antejavor.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://antejavor.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-04T09:19:40+00:00</updated><id>https://antejavor.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">How do LLMs reason?</title><link href="https://antejavor.github.io/blog/2025/how-do-LLMs-reason/" rel="alternate" type="text/html" title="How do LLMs reason?"/><published>2025-04-04T00:00:00+00:00</published><updated>2025-04-04T00:00:00+00:00</updated><id>https://antejavor.github.io/blog/2025/how-do-LLMs-reason</id><content type="html" xml:base="https://antejavor.github.io/blog/2025/how-do-LLMs-reason/"><![CDATA[<h2 id="the-mighty-llms">The mighty LLMs</h2> <p>We live in fascinating times of mighty LLMs that can help boost productivity and help with day-to-day tasks. The speed of adoption of LLMs is a story that will constantly be compared to everything that came before and after the LLMs.</p> <p>It is essential to highlight and understand the fundamentals and flaws of this technology. Let’s jump to the big question around LLMs that became quite controversial, <strong>can they reason?</strong> My short answer is <strong>no</strong>. Let’s dive into why.</p> <h2 id="reasoning">Reasoning</h2> <p>To even talk about the reasoning, we would first have to define it and agree on what reasoning is. Let’s distill this into something we refer to as a typical thinking process where you are presented with some information and can think about that information and conclude something.</p> <p>In the practical example, it would be something like this: <strong>The S&amp;P 500 sank 15% percent in the three months</strong>.</p> <p>Now, in your brain, you start to ponder that information and connect it to the events that happened in that period. Some notable things that could pop into your mind are the Trump tariffs, lower-than-expected BDP in the US economy, global conflicts, etc.</p> <p>In a pure form, this is reasoning about specific information in your brain. If I follow up on the question and ask: “What will the stock market do tomorrow?” you will probably start thinking hard about this.</p> <p>In other words, you would read projections, news, and global tax and tariff policies and invest time and effort into trying to answer this. You would construct a thesis that works under different assumptions.</p> <p>Usually, the more effort you invest, the higher the probability you will be correct.</p> <p>In a pure form, it is reasoning on the information you were given, had in your memory or acquired over the web.</p> <p>The nice thing about reasoning is that our brain is the ultimate reasoning powerhouse (most of the time :D), and it runs on beer and bananas, it does not require <a href="https://www.theverge.com/24066646/ai-electricity-energy-watts-generative-consumption">1 Megawatt of electricity to be trained</a></p> <h2 id="reasoning-and-llms">Reasoning and LLMs</h2> <p>LLMs <strong>cannot</strong> reason, they can <strong>mimic</strong> reasoning that sometimes works and sometimes does not. You can call it primitive, low reasoning, or use any other way to package it, but it is far away from your reasoning capabilities.</p> <p>This starts with the basis of what LLMs actually are, they are a super capable text predictor that has a lot of memory. Memory is constructed based on what they have been trained on, and small “thinking memory” is the context you are passing with your question.</p> <p>Let’s put this into practical example if your LLM was trained on this data:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>We have different colors for objects: blue, red, green, purple, yellow, etc.
The ball can be red.
The ball can be blue.
The car can be purple.
The flag can be purple.
</code></pre></div></div> <p>Now, if I ask: “What color can a ball be? Give me three answers.”</p> <p>The LLM computation will give a higher probability to the obvious answers:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. The ball can be red.
2. The ball can be blue.
3. The Ball can be ...
</code></pre></div></div> <p>The third answer would be some of the possible colors, because more objects are purple and purple has a higher frequency, it would probably give the third answer as purple. But we all know that it can be any color since it is not specified.</p> <p>If you want more details about this, one of the best explainers about why LLMs is a text predictor, I would recommend jumping on the 3Blue1Brown channel and watching a <a href="https://youtu.be/aircAruvnKk?si=6akRbRcsc2nOE_G2">series about neural networks</a>, specifically about the LLMs and transformers.</p> <p>By design, they are not made to think but rather to predict the next word (token, to be exact).</p> <p>Well <a href="https://platform.openai.com/docs/guides/reasoning?api-mode=chat">OpenAI docs seem to suggest otherwise</a>.</p> <p>This is a snippet from their docs page:</p> <blockquote> <p>Reasoning models, like OpenAI o1 and o3-mini, are new large language models trained with reinforcement learning to perform complex reasoning. Reasoning models think before they answer, producing a long internal chain of thought before responding to the user. Reasoning models excel in complex problem solving, coding, scientific reasoning, and multi-step planning for agentic workflows.</p> </blockquote> <p>The image from the same docs page illustrates this process: <img src="/assets/img/blog-how-llms-reasone/reasoning_flow.png" alt="reasoning" width="900"/></p> <p>They are saying their models can perform complex reasoning.</p> <p>Well, they do not reason. They generate a thesis on how to answer questions, summarize that generation into reasoning and output, and chain this multiple times, which improves the text generation and makes it more successful. Something you will hear as <strong>Chain of Thoughts</strong>.</p> <p>So, they are <strong>mimicking</strong> reasoning where you generate text related to the text you are asking for to try to come up with a solution. It is like a person saying the facts out loud when someone asks you a question or creates a theory about something and uses theory construction to present the facts. At any step of the process, you are hoping the theory won’t go sideways.</p> <p>If we go back to the LLM that was trained on ball colors:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Question asked: What color can a ball be?

Chain of thought:
The ball can be blue
The ball can be red

Output:

Therefore, the ball can be red or blue.
</code></pre></div></div> <p>This would be a good Chain of Thoughts:</p> <p>Here is how it could fail:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Question asked: What color can a ball be? Give me three answers
Chain of thought:

The ball can be blue
The ball can be red
The car can be purple.


Therefore, the ball can be blue, red, and, I guess, purple.


</code></pre></div></div> <p>If you do not trust my examples, give a shot to your <strong>reasoning</strong> LLM, I got this on ChatGPT with reasoning:</p> <p><img src="/assets/img/blog-how-llms-reasone/chat-gpt-reasoning.png" alt="chat-gpt-reasoning" width="900"/></p> <p>If you do not trust me, one of the best men to describe this is <a href="https://www.youtube.com/watch?v=N09C6oUQX5M">Yann LeCunn</a>, Chief AI Scientist at Meta.</p> <p>I admire the engineering behind modern LLMs and find them helpful. If you provide the proper context, they won’t hallucinate. That is why we, in <a href="https://memgraph.com/docs/ai-ecosystem">Memgraph</a> are focused on building cool Graph and LLM-powered AI apps.</p> <p>As they say, the harder it is to explain and argue how bad or incapable technology is, the better it is. <a href="https://en.wikipedia.org/wiki/Clarke%27s_three_laws">Arthur C. Clarke’s Third Law</a>:</p> <blockquote> <p>Any sufficiently advanced technology is indistinguishable from magic.</p> </blockquote>]]></content><author><name></name></author><category term="AI"/><category term="LLMs,"/><category term="AI"/><summary type="html"><![CDATA[Understanding what reasoning is and how LLMs can reason.]]></summary></entry></feed>